{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f341bae7",
   "metadata": {},
   "source": [
    "# Shinkansen Travel Experience\n",
    "\n",
    "---------------\n",
    "## **Context:**\n",
    "---------------\n",
    "This problem statement is based on the Shinkansen Bullet Train in Japan, and passengers’ experience with that mode of travel. This machine-learning exercise aims to determine the relative importance of each parameter with regard to their contribution to the passengers’ overall travel experience. The dataset contains a random sample of individuals who travelled on this train. The on-time performance of the trains along with passenger information is published in a file named ‘Traveldata_train.csv’.  These passengers were later asked to provide their feedback on various parameters related to the travel, along with their overall experience. These collected details are made available in the survey report labelled ‘Surveydata_train.csv’.\n",
    "\n",
    "In the survey, each passenger was explicitly asked whether they were satisfied with their overall travel experience or not, and that is captured in the data of the survey report under the variable labelled ‘Overall_Experience’. \n",
    "\n",
    "The objective of this problem is to understand which parameters play an important role in swaying passenger feedback towards a positive scale. You are provided test data containing the travel data and the survey data of passengers. Both the test data and the train data are collected at the same time and belong to the same population.\n",
    "\n",
    "---------------\n",
    "## **Goal:**\n",
    "---------------\n",
    "The goal of the problem is to predict whether a passenger was satisfied or not considering his/her overall experience of traveling on the Shinkansen Bullet Train.\n",
    "\n",
    "---------------\n",
    "## **Dataset:**\n",
    "---------------\n",
    "The problem consists of 2 separate datasets: Travel data & Survey data. Travel data has information related to passengers and attributes related to the Shinkansen train, in which they traveled. The survey data is aggregated data of surveys indicating the post-service experience. You are expected to treat both these datasets as raw data and perform any necessary data cleaning/validation steps as required.\n",
    "\n",
    "The data has been split into two groups and provided in the Dataset folder. The folder contains both train and test data separately.\n",
    "\n",
    "Train_Data\n",
    "Test_Data\n",
    "\n",
    "Target Variable: Overall_Experience (1 represents ‘satisfied’, and 0 represents ‘not satisfied’)\n",
    "\n",
    "The training set can be used to build your machine-learning model. The training set has labels for the target column - Overall_Experience.\n",
    "\n",
    "The testing set should be used to see how well your model performs on unseen data. For the test set, it is expected to predict the ‘Overall_Experience’ level for each participant.\n",
    "\n",
    "Data Dictionary:\n",
    "All the data is self-explanatory. The survey levels are explained in the Data Dictionary file.\n",
    "\n",
    "Submission File Format: You will need to submit a CSV file with exactly 35,602 entries plus a header row. The file should have exactly two columns\n",
    "\n",
    "ID\n",
    "Overall_Experience (contains 0 & 1 values, 1 represents ‘Satisfied’, and 0 represents ‘Not Satisfied’)\n",
    "\n",
    "---------------\n",
    "## **Evaluation Criteria:**\n",
    "---------------\n",
    "Accuracy Score: The evaluation metric is simply the percentage of predictions made by the model that turned out to be correct. This is also called the accuracy of the model. It will be calculated as the total number of correct predictions (True Positives + True Negatives) divided by the total number of observations in the dataset.\n",
    " \n",
    "In other words, the best possible accuracy is 100% (or 1), and the worst possible accuracy is 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943251ec",
   "metadata": {},
   "source": [
    "Sometimes, the installation of the surprise library, which is used to build recommendation systems, faces issues in Jupyter. To avoid any issues, it is advised to use **Google Colab** for this project.\n",
    "\n",
    "Let's start by mounting the Google drive on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d53e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Python libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For implementing matrix factorization based recommendation system\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "from collections import defaultdict\n",
    "\n",
    "# For implementing cross validation\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97930d78",
   "metadata": {},
   "source": [
    "### **Loading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8605454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the datasets. Train dataset files\n",
    "Traveldata_train = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Shinkansen Travel Experience\\Traveldata_train.csv\")\n",
    "Surveydata_train = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Shinkansen Travel Experience\\Surveydata_train.csv\")\n",
    "Traveldata_test = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Shinkansen Travel Experience\\Traveldata_test.csv\")\n",
    "Surveydata_test = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Shinkansen Travel Experience\\Surveydata_test.csv\")\n",
    "Data_Dictionary = pd.read_csv(r\"C:\\Users\\pc\\Desktop\\Shinkansen Travel Experience\\Data_Dictionary.csv\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the data is loaded correctly\n",
    "print(Traveldata_train.shape)\n",
    "print(Surveydata_train.shape)\n",
    "print(Traveldata_test.shape)\n",
    "print(Surveydata_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee629f17",
   "metadata": {},
   "source": [
    "### **Exploring the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06269778",
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveldata_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ebdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Surveydata_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveldata_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Surveydata_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "Traveldata_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568eaefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Surveydata_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values using a heatmap for better understanding:\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(Traveldata_train.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('Missing Values in Traveldata_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance in the target variable (Overall_Experience)\n",
    "ptrain_data[\"Overall_Experience\"].value_counts(normalize=True)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Overall_Experience', data=ptrain_data, palette='viridis')\n",
    "plt.title('Class Distribution of Overall Experience')\n",
    "plt.xlabel('Overall Experience')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8cf2d9",
   "metadata": {},
   "source": [
    "#### **Now lets merge the datasets the train and test datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Datasets\n",
    "train_data = pd.merge(Traveldata_train, Surveydata_train, on=\"ID\")\n",
    "test_data = pd.merge(Traveldata_test, Surveydata_test, on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664989eb",
   "metadata": {},
   "source": [
    "#### **Cleaning the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274f2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the merged dataset\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing missing values\n",
    "train_data.fillna(train_data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with missing values\n",
    "# train_data = train_data.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e031e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the merged dataset\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical\n",
    "from sklearn.preprocessing import LabelEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1680a8",
   "metadata": {},
   "source": [
    "#### **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e87590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the target variable \n",
    "sns.countplot(x=\"Overall_Experience\", data=train_data)\n",
    "sns.boxplot(x=\"Overall_Experience\", y=\"some_feature\", data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa280e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify important features using correlation or feature importance techniques. \n",
    "# For example, using correlation matrix\n",
    "correlation_matrix = train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap:\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(train_data.corr(), annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for selected features:\n",
    "sns.pairplot(train_data, hue=\"Overall_Experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0bef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical and numerical features separately for better analysis.\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"Categorical Features: \", categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48da558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider using OneHotEncoder for categorical variables with more than two categories.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features using StandardScaler or MinMaxScaler for models sensitive to feature scaling (e.g., Logistic Regression, SVM).\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d644c217",
   "metadata": {},
   "source": [
    "#### **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60508371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_data.drop(columns=[\"Overall_Experience\"])\n",
    "y = train_data[\"Overall_Experience\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88585f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classification model (e.g., Logistic Regression, Random Forest, Gradient Boosting):\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set:\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance visualization for Random Forest:\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feature_importances.nlargest(10).plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation to ensure the model's performance is consistent:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(\"Cross-Validation Accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ed2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ebaad5a",
   "metadata": {},
   "source": [
    "#### **Test the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeeb8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the Overall_Experience for the test dataset:\n",
    "test_predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f802d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in the required format:\n",
    "submission = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b99dd2",
   "metadata": {},
   "source": [
    "#### **Iteration and Improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different models (e.g., XGBoost, LightGBM).\n",
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained XGBoost model to predict the Overall_Experience for the test dataset:\n",
    "test_predictions_xgb = xgb_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in the required format:\n",
    "submission_xgb = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions_xgb})\n",
    "submission_xgb.to_csv(\"submission_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_val)\n",
    "print(\"Best Model Accuracy:\", accuracy_score(y_val, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model to predict the Overall_Experience for the test dataset:\n",
    "test_predictions_best = best_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a531c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a comparison of model performances (e.g., Random Forest vs. XGBoost vs. Voting Classifier) in a table or bar chart.\n",
    "model_performance = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'XGBoost', 'Best Model'],\n",
    "    'Accuracy': [accuracy_score(y_val, y_pred), accuracy_score(y_val, y_pred_xgb), accuracy_score(y_val, y_pred_best)]\n",
    "})\n",
    "model_performance.plot(x='Model', y='Accuracy', kind='bar', legend=False)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model predictions in the required format:\n",
    "submission_best = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions_best})\n",
    "submission_best.to_csv(\"submission_best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34152c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions in the required format:\n",
    "submission_best = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions_best})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature selection techniques to improve performance.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selector = SelectFromModel(RandomForestClassifier(random_state=42))\n",
    "selector.fit(X_train, y_train)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_val_selected = selector.transform(X_val)\n",
    "X_test_selected = selector.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model again with selected features:\n",
    "model_selected = RandomForestClassifier(random_state=42)\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "y_pred_selected = model_selected.predict(X_val_selected)\n",
    "print(\"Selected Features Model Accuracy:\", accuracy_score(y_val, y_pred_selected))\n",
    "# Use the trained model with selected features to predict the Overall_Experience for the test dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model with selected features to predict the Overall_Experience for the test dataset: \n",
    "test_predictions_selected = model_selected.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions in the required format:\n",
    "submission_best = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions_best})\n",
    "submission_best.to_csv(\"submission_best.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ensemble methods to combine predictions from multiple models.\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('xgb', XGBClassifier(random_state=42))\n",
    "], voting='soft')\n",
    "voting_model.fit(X_train, y_train)\n",
    "y_pred_voting = voting_model.predict(X_val)\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_val, y_pred_voting))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the voting model to predict the Overall_Experience for the test dataset:\n",
    "test_predictions_voting = voting_model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead1564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions in the required format:\n",
    "submission = pd.DataFrame({\"ID\": test_data[\"ID\"], \"Overall_Experience\": test_predictions})\n",
    "submission.to_csv(\"submission_voting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4e7ab",
   "metadata": {},
   "source": [
    "#### **Submition and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc54f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the SHAP explainer for your model\n",
    "explainer = shap.TreeExplainer(model)  # Use TreeExplainer for tree-based models like Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4724115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values for the validation dataset\n",
    "shap_values = explainer.shap_values(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], X_val)  # For binary classification, use shap_values[1] for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the submission file matches the required format.\n",
    "submission.head()\n",
    "submission.info()\n",
    "submission.describe()\n",
    "submission.isnull().sum()\n",
    "submission.duplicated().sum()\n",
    "submission.to_csv(\"submission_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the prediction for a single instance\n",
    "instance = X_val.iloc[0].values  # Select a single row\n",
    "explanation = explainer.explain_instance(instance, model.predict_proba)\n",
    "\n",
    "# Visualize the explanation\n",
    "explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the file and evaluate the accuracy score.\n",
    "# The accuracy score will be evaluated based on the test dataset and the submission file.\n",
    "# The evaluation metric will depend on the competition or task requirements.\n",
    "# For example, if the task is a classification problem, accuracy score can be used.\n",
    "# If the task is a regression problem, RMSE or MAE can be used.\n",
    "# The evaluation metric can be calculated using the true labels and predicted labels.\n",
    "# For example, if the true labels are in a variable called y_true and predicted labels are in y_pred:\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error\n",
    "# y_true = test_data[\"Overall_Experience\"]\n",
    "# y_pred = test_predictions\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "# rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "# mae = mean_absolute_error(y_true, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"RMSE:\", rmse)\n",
    "# print(\"MAE:\", mae)\n",
    "# The evaluation metric can be used to compare different models and select the best one.\n",
    "# The best model can be used to make predictions on the test dataset and submit the results.\n",
    "# The final submission file can be submitted to the competition or task platform for evaluation.\n",
    "# The evaluation results can be used to analyze the model performance and improve it further.\n",
    "# The model can be improved by using more advanced techniques such as deep learning, transfer learning, or reinforcement learning.\n",
    "# The model can also be improved by using more data, better features, or better hyperparameters.\n",
    "# The model can be improved by using more advanced techniques such as deep learning, transfer learning, or reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
